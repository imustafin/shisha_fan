{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Posts and Comments from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEFORE_START = '\\0'\n",
    "AFTER_END = '\\1'\n",
    "def preproc(s):\n",
    "    s = BEFORE_START + s + AFTER_END\n",
    "    s = re.sub(r'\\[.*\\|.*\\]', ' ', s)\n",
    "    s = re.sub(r'[¬´¬ª]', '\"', s)\n",
    "    return s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text samples: 1747\n"
     ]
    }
   ],
   "source": [
    "with open('data/kalikfan.json', 'r') as f:\n",
    "    kalikfan = json.load(f)\n",
    "\n",
    "SEQ_LENGTH = 10    \n",
    "\n",
    "texts = []\n",
    "for p in kalikfan:\n",
    "    if len(p['text']) > SEQ_LENGTH:\n",
    "        texts.append(p['text'])\n",
    "        \n",
    "    for c in p['comments']:\n",
    "        if c['likesCount'] > 2 and len(c['text']) > SEQ_LENGTH:\n",
    "            texts.append(c['text'])\n",
    "\n",
    "texts = [preproc(x) for x in texts]\n",
    "print('Number of text samples:', len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shisha Learning ü§ôü§ôü§ô\n",
    "From https://stackabuse.com/text-generation-with-python-and-tensorflow-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of text samples: 1747\n",
      "Total vocab: 820\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(texts))))\n",
    "char_to_num = dict((c, i) for i, c in enumerate(chars))\n",
    "num_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "input_len = len(texts)\n",
    "vocab_len = len(chars)\n",
    "print (\"Total number of text samples:\", input_len)\n",
    "print (\"Total vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 285401\n"
     ]
    }
   ],
   "source": [
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for text in texts:\n",
    "    t = [BEFORE_START] * (SEQ_LENGTH - 1) + list(text) + [AFTER_END] * SEQ_LENGTH\n",
    "    for i in range(0, len(t) - SEQ_LENGTH):\n",
    "        in_seq = t[i:i + SEQ_LENGTH]\n",
    "        out = t[i + SEQ_LENGTH]\n",
    "        x_data.append([char_to_num[x] for x in in_seq])\n",
    "        y_data.append(char_to_num[out])\n",
    "        \n",
    "n_patterns = len(x_data)\n",
    "print (\"Total Patterns:\", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(x_data, (n_patterns, SEQ_LENGTH, 1))\n",
    "X = X/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np_utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(CuDNNLSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(CuDNNLSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"model_weights_saved.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 285401 samples\n",
      "Epoch 1/5\n",
      "285320/285401 [============================>.] - ETA: 0s - loss: 2.3848\n",
      "Epoch 00001: loss did not improve from 2.20645\n",
      "285401/285401 [==============================] - 103s 361us/sample - loss: 2.3848\n",
      "Epoch 2/5\n",
      "285340/285401 [============================>.] - ETA: 0s - loss: 2.4561\n",
      "Epoch 00002: loss did not improve from 2.20645\n",
      "285401/285401 [==============================] - 99s 345us/sample - loss: 2.4561\n",
      "Epoch 3/5\n",
      "285320/285401 [============================>.] - ETA: 0s - loss: 2.4875\n",
      "Epoch 00003: loss did not improve from 2.20645\n",
      "285401/285401 [==============================] - 99s 346us/sample - loss: 2.4874\n",
      "Epoch 4/5\n",
      "285280/285401 [============================>.] - ETA: 0s - loss: 2.4887\n",
      "Epoch 00004: loss did not improve from 2.20645\n",
      "285401/285401 [==============================] - 99s 348us/sample - loss: 2.4887\n",
      "Epoch 5/5\n",
      "285400/285401 [============================>.] - ETA: 0s - loss: 2.4875\n",
      "Epoch 00005: loss did not improve from 2.20645\n",
      "285401/285401 [==============================] - 99s 347us/sample - loss: 2.4875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f798c814890>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=100, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"model_weights_saved.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(pattern):\n",
    "    result = []\n",
    "    while True:\n",
    "        if len(result) > 500:\n",
    "            result += '@'\n",
    "            break\n",
    "        x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        x = x / float(vocab_len)\n",
    "        prediction = model.predict(x, verbose=0)\n",
    "        res = np.random.choice(chars, 1, False, prediction[0])[0]\n",
    "        \n",
    "        if res == AFTER_END:\n",
    "            break\n",
    "        result += res\n",
    "        \n",
    "        seq_in = [num_to_char[value] for value in pattern]\n",
    "\n",
    "        pattern.append(char_to_num[res])\n",
    "        pattern = pattern[1:]\n",
    "        \n",
    "    return ''.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    s = [char_to_num[BEFORE_START]] * SEQ_LENGTH\n",
    "    return pred(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–∞–ª—Å –∫—Ö—Ç—Ñ–∞–∞ —Å–µ—ã–µ–ª –ø–∫—Ä—Ç—Ä–∏ ü§™üöåüôè –Ω–∞ –∑–µ–∑—Ç—å –æ –Ω–∞–ø—Ç–æ—è—â–∏—å)—Ä–æ–∏ –∞—Ä—è–¥—Çüò±üí®ü•∞–æ—É–¥—ã–ª–∞—Ä–µ—Ç–∞—Å—å,\n"
     ]
    }
   ],
   "source": [
    "print(test())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
